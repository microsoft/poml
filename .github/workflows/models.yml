name: Use GitHub Models
on:
  workflow_dispatch:

permissions:
  models: read  # allow calling the Models inference API

jobs:
  call-llm:
    runs-on: ubuntu-latest
    steps:
      - name: Ask a model
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Making request to GitHub Models API..."
          response=$(curl -sS -w "HTTP_STATUS:%{http_code}" \
            https://models.github.ai/inference/chat/completions \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -d '{
              "model": "openai/gpt-5",
              "messages": [
                {"role": "system", "content": "You are a concise assistant."},
                {"role": "user", "content": "Explain recursion in 2 sentences."}
              ],
              "max_tokens": 300
            }')
          
          # Extract HTTP status code
          http_status=$(echo "$response" | grep -o "HTTP_STATUS:[0-9]*" | cut -d: -f2)
          # Extract response body (everything before HTTP_STATUS)
          response_body=$(echo "$response" | sed 's/HTTP_STATUS:[0-9]*$//')
          
          echo "HTTP Status: $http_status"
          echo "Response:"
          echo "$response_body" | jq . || echo "$response_body"
          
          # Exit with error if not successful
          if [ "$http_status" -ne 200 ]; then
            echo "Request failed with status $http_status"
            exit 1
          fi